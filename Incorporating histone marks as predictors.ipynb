{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, Merge, Reshape \n",
    "from keras.layers.core import Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io as sio\n",
    "\n",
    "import h5py\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "from deepsea import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X, dmy = loaddata(\"data/deepsea/\", \"test\")\n",
    "train_X, dmy = loaddata(\"data/deepsea/\", \"train\")\n",
    "valid_X, dmy = loaddata(\"data/deepsea/\", \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_marks = np.load(\"data/deepersea/testx.npy\")\n",
    "train_marks = np.load(\"data/deepersea/trainx.npy\")\n",
    "valid_marks = np.load(\"data/deepersea/validx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_Y = np.load(\"data/deepersea/testy.npy\") \n",
    "train_Y = np.load(\"data/deepersea/trainy.npy\") \n",
    "valid_Y = np.load(\"data/deepersea/validy.npy\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a data generator\n",
    "\n",
    "Keras model can be trained both on a numpy array and a keras Iterator.  \n",
    "Here we implement our own custom iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataIterator(Iterator):\n",
    "    \n",
    "    def __init__(self, seq, mark, y, batch_size=32, shuffle=False, seed=None):\n",
    "        self.seq = seq\n",
    "        self.mark = mark\n",
    "        self.y = y\n",
    "        super(DataIterator, self).__init__(mark.shape[0], batch_size, shuffle, seed)\n",
    "        \n",
    "    def next(self):\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch.\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        \n",
    "        seqindex =  [i%self.seq.shape[0] for i in index_array]\n",
    "        \n",
    "        seq_batch = self.seq[seqindex, :]\n",
    "        mark_batch = self.mark[index_array, :]\n",
    "        \n",
    "        if self.y is None:\n",
    "            return [seq_batch, mark_batch]\n",
    "        batch_y = self.y[index_array]\n",
    "        return  [seq_batch, mark_batch], batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid = DataIterator(valid_X, valid_marks, valid_Y, batch_size=256)\n",
    "train = DataIterator(train_X, train_marks, train_Y, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DeeperSEA():\n",
    "    nkernels = [320,480,960]\n",
    "    nhidden = [15, 15]\n",
    "    in_size1 = (1,1000,4)\n",
    "    in_size2 = (11, )\n",
    "    l2_lam = 5e-07 \n",
    "    l1_lam = 1e-08 \n",
    "    \n",
    "    #Generating convolution arm \n",
    "    model1 = Sequential()\n",
    "    model1.add(Conv2D(nkernels[0], kernel_size=(1,8), strides=(1,1), padding='same', input_shape=in_size1, kernel_regularizer=regularizers.l2(l2_lam)))\n",
    "    model1.add(Activation('relu'))\n",
    "    model1.add(MaxPooling2D(pool_size=(1,4), strides=(1,4)))\n",
    "    model1.add(Dropout(0.2))\n",
    "    \n",
    "    model1.add(Conv2D(nkernels[1], kernel_size=(1,8), strides=(1,1), padding='same', kernel_regularizer=regularizers.l2(l2_lam)))\n",
    "    model1.add(Activation('relu'))\n",
    "    model1.add(MaxPooling2D(pool_size=(1,4), strides=(1,4)))\n",
    "    model1.add(Dropout(0.2))\n",
    "\n",
    "    model1.add(Conv2D(nkernels[1], kernel_size=(1,8), strides=(1,1), padding='same', kernel_regularizer=regularizers.l2(l2_lam)))\n",
    "    model1.add(Activation('relu'))\n",
    "    model1.add(Dropout(0.5))\n",
    "    model1.add(Flatten())\n",
    "    print model1.output_shape \n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(nhidden[0], kernel_regularizer=regularizers.l1(l1_lam), input_shape=in_size2))\n",
    "    model2.add(Activation('relu'))\n",
    "    model2.add(Dense(nhidden[1], kernel_regularizer=regularizers.l1(l1_lam)))\n",
    "    model2.add(Activation('relu'))\n",
    "    print model2.output_shape\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    #TODO fix deprecation\n",
    "    model.add(Merge([model1, model2], mode=\"concat\"))\n",
    "    print model.output_shape\n",
    "    \n",
    "    model.add(Dense(919, kernel_regularizer=regularizers.l1(l1_lam)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, kernel_regularizer=regularizers.l1(l1_lam)))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 29760)\n",
      "(None, 15)\n",
      "(None, 29775)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiranumn/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:37: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model = DeeperSEA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"deepsea-{epoch:02d}-{val_loss:.2f}-full.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = model.fit_generator(train, train_marks.shape[0]/256, epochs=10, verbose=0,\n",
    "                             validation_data=valid, validation_steps=valid_marks.shape[0]/256,\n",
    "                             callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
